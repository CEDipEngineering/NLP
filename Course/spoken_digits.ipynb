{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversão Audio-para-Texto\n",
    "\n",
    "**Objetivo: aplicar analisadores no domínio do tempo para fazer classificação**\n",
    "\n",
    "Neste problema, recebemos como entrada uma gravação de um dígito falado (em inglês) e queremos saber qual dígito foi falado.\n",
    "\n",
    "\n",
    "A tarefa para este trabalho é:\n",
    "\n",
    "1. Desenvolver um sistema com aprendizado profundo capaz de realizar a tarefa de identificar qual é o dígito falado na gravação\n",
    "2. Avaliar o sistema (possivelmente otimizando hiperparâmetros) e compará-lo com o sistema \"baseline\" mostrado neste notebook\n",
    "3. Usar figuras e outros recursos para explicitar mais detalhes sobre o funcionamento do sistema, mostrando evidências de que ele não está somente usando coincidências presentes na base de dados (por exemplo: talvez algum dígito tenha gravações mais longas ou mais curtas...)\n",
    "\n",
    "Roteiro:\n",
    "\n",
    "1. Clique em \"Run All\" para executar o código exemplo, e baixar o dataset (a primeira execução pode levar por volta de 20min).\n",
    "2. Leia atentamente o código que foi fornecido como exemplo. Veja como ele funciona, questione as dimensões das matrizes, etc.\n",
    "3. Use uma rede neural profunda com processamento no domínio do tempo (LSTM, RNN, atenção, etc...) para propor um novo classificador.\n",
    "4. Depois de testar o seu classificador, use figuras e outras observações para encontrar quais são os casos em que o classificador mais erra e, se possível, identificar qual é o motivo do erro\n",
    "5. Modifique seu classificador de forma a corrigir os erros que você encontrou anteriormente.\n",
    "\n",
    "Algumas anotações importantes:\n",
    "\n",
    "1. Uma representação de timbre que pode ser útil para nossa busca usa MFCCs (Mel-Frequency Cepstral Coefficients).\n",
    "2. MFCCs podem ser entendidos como \"a energia que passa por filtros inspirados na audição humana\".\n",
    "3. Outra interpretação para MFCCs é: \"um vetor que mapeia sons em $\\mathbb{R}^N$ de tal forma que sons perceptualmente próximos ficam em posições\n",
    "4. Um pequeno quadro (512 amostras?) de áudio está relacionado a 12 MFCCs\n",
    "5. Na implementação atual, usamos a média dos MFCCs como representação do áudio. Mude isso na sua implementação, porque a média dos MFCCs é uma representação terrível!\n",
    "6. O código atual já encontra os MFCCs em pequenos quadros do áudio de entrada. Não precisa modificar essa parte.\n",
    "7. A divisão dos dados em conjunto de treino e teste leva em consideração que um falante que está no treino não pode estar também no teste (pois isso seria overfitting).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')\n",
    "import numpy as np\n",
    "import librosa\n",
    "import hub\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import NMF, PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import Isomap, TSNE\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, TimeDistributed, Embedding, GlobalAveragePooling1D, Flatten, SimpleRNN, GRU, Dropout, LSTM, Bidirectional\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import os\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hub://activeloop/spoken_mnist loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "ds = hub.load(\"hub://activeloop/spoken_mnist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como acessar a estrutura ds?\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acessando os dados via numpy\n",
    "y = ds.audio[0].numpy()\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotando um arquivo de audio\n",
    "fs = 8*10**3 # 8 kHz sample rate (fonte: https://github.com/Jakobovski/free-spoken-digit-dataset)\n",
    "t = np.arange(len(y))/fs\n",
    "plt.figure(figsize=(5,2))\n",
    "plt.plot(t,y)\n",
    "plt.xlabel('Tempo (s)')\n",
    "plt.ylabel('Amostras')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-processamento (transformar áudio em vetores)\n",
    "\n",
    "AVISO: O DATASET É BAIXADO EM TEMPO DE EXECUÇÃO, ENTÃO A PRIMEIRA EXECUÇÃO DESTA CÉLULA DEVE LEVAR POR VOLTA DE 20min (depois, os dados ficam em cache e o acesso é mais rápido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_coef = 12\n",
    "fs = 8000\n",
    "X = np.zeros( (3000, 12) )\n",
    "y = np.zeros( (3000, ))\n",
    "speaker = []\n",
    "\n",
    "for i in range(len(ds.audio)):\n",
    "    x_ = ds.audio[i].numpy()\n",
    "    x_.shape = (x_.shape[0],)\n",
    "    \n",
    "    # Normalização: transformar para média zero e variância 1\n",
    "    x_ -= np.mean(x_)\n",
    "    x_ /= np.std(x_)\n",
    "\n",
    "    # Capturar MFCCs\n",
    "    mfcc = librosa.feature.mfcc(y=x_, sr=fs, n_mfcc=n_coef, n_fft=512, hop_length=128)\n",
    "\n",
    "    # Guardar média dos MFCCs\n",
    "    # Se for usar mfccs diretamente (sem ser a média),\n",
    "    # precisa tirar o cálculo da média\n",
    "    # Dica: é para usar mfccs diretamente!\n",
    "    # Dica 2: lembre-se de fazer zero-padding para deixar as observações todas com o mesmo tamanho\n",
    "    # na dimensão do tempo, para poder usar o Keras\n",
    "    X[i,:] = np.mean(mfcc, axis=1)\n",
    "    y[i] = ds.labels[i].numpy()\n",
    "    speaker.append(ds.speakers[i].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "y.shape = (y.shape[0],)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separação em treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separação em treino e teste evitando que um falante fique tanto no treino quanto no teste\n",
    "gss  = GroupShuffleSplit(n_splits=1, train_size=.5, random_state=42)\n",
    "for idx_train_, idx_test_ in gss.split(X, y, speaker):\n",
    "    idx_train = idx_train_\n",
    "    idx_test = idx_test_\n",
    "    \n",
    "X_train = X[idx_train,:]\n",
    "X_test = X[idx_test,:]\n",
    "y_train = y[idx_train]\n",
    "y_test = y[idx_test]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline do classificador (baseline!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "y_pred = lr.predict(X_test_scaled)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maxlen = 50\n",
    "# X_2 = np.zeros( (3000, 12, maxlen) )\n",
    "# y = np.zeros( (3000, ))\n",
    "# speaker = []\n",
    "\n",
    "# for i in range(10): #range(len(ds.audio)):\n",
    "#     x_ = ds.audio[i].numpy()\n",
    "#     x_.shape = (x_.shape[0],)\n",
    "    \n",
    "#     # Normalização: transformar para média zero e variância 1\n",
    "#     x_ -= np.mean(x_)\n",
    "#     x_ /= np.std(x_)\n",
    "\n",
    "#     # Capturar MFCCs\n",
    "#     mfcc = librosa.feature.mfcc(y=x_, sr=fs, n_mfcc=n_coef, n_fft=512, hop_length=128)\n",
    "#     print(mfcc.shape)\n",
    "#     # Guardar média dos MFCCs\n",
    "#     # Se for usar mfccs diretamente (sem ser a média),\n",
    "#     # precisa tirar o cálculo da média\n",
    "#     # Dica: é para usar mfccs diretamente!\n",
    "#     # Dica 2: lembre-se de fazer zero-padding para deixar as observações todas com o mesmo tamanho\n",
    "#     # na dimensão do tempo, para poder usar o Keras\n",
    "#     # X[i,:] = pad_sequences(mfcc, maxlen=maxlen)\n",
    "#     for j in range(len(mfcc)):\n",
    "#         X[i,j] = pad_sequences([mfcc[j]], maxlen=maxlen)\n",
    "#     y[i] = ds.labels[i].numpy()\n",
    "#     speaker.append(ds.speakers[i].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
